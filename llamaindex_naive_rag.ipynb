{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNb/OQEcMi3anZ0wXQo6SBy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wentilabs/tutorials/blob/main/llamaindex_naive_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LlamaIndex Tutorial\n",
        "source: [https://https://docs.llamaindex.ai/en/stable/examples/chat_engine/chat_engine_openai.html#get-started-in-5-lines-of-code)](https://https://docs.llamaindex.ai/en/stable/examples/chat_engine/chat_engine_openai.html#get-started-in-5-lines-of-code)"
      ],
      "metadata": {
        "id": "l2antzohYncG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install LlamaIndex"
      ],
      "metadata": {
        "id": "FaLIqCTIP9pO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFTf4WqjPnaV"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Sample Data from LlamaIndex"
      ],
      "metadata": {
        "id": "Yv5U6fFXQB6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
      ],
      "metadata": {
        "id": "1a4cDX60P1tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import your own OpenAI API"
      ],
      "metadata": {
        "id": "7NQ7piZUT1Qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "#os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = #Insert your own OpenAI API KEY"
      ],
      "metadata": {
        "id": "OHTRSOJ2RpDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load data and build index**\n",
        "\n",
        "Get started in 5 lines of code"
      ],
      "metadata": {
        "id": "II_TZJ_WP5id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "# Necessary to use the latest OpenAI models that support function calling API\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
        ")\n",
        "data = SimpleDirectoryReader(input_dir=\"./data/paul_graham/\").load_data()\n",
        "index = VectorStoreIndex.from_documents(data, service_context=service_context)"
      ],
      "metadata": {
        "id": "zYiUXWwBQIrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure Chat Engine"
      ],
      "metadata": {
        "id": "Y7MXE9UVVkwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index.as_chat_engine(chat_mode=\"openai\", verbose=True)"
      ],
      "metadata": {
        "id": "FE9wbvhVQirI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interact with your naive RAG model"
      ],
      "metadata": {
        "id": "iWlRLVMMVorr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_engine.chat(\"Hi\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tnNhjvDTo0_",
        "outputId": "742e8791-1569-4fdf-c406-665737d3b68a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Hi\n",
            "Hello! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_engine.chat(\n",
        "    \"Use the tool to answer: Who did Paul Graham hand over YC to?\"\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQ0WWSTzTtFG",
        "outputId": "e12a192d-5ad4-4b57-cc6c-5b944c36359e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Use the tool to answer: Who did Paul Graham hand over YC to?\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\n",
            "  \"input\": \"Who did Paul Graham hand over YC to?\"\n",
            "}\n",
            "Got output: Paul Graham handed over YC to Sam Altman.\n",
            "========================\n",
            "\n",
            "Paul Graham handed over Y Combinator (YC) to Sam Altman.\n"
          ]
        }
      ]
    }
  ]
}